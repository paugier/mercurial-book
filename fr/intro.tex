\chapter{Introduction}
\label{chap:intro}

\section{A propros de la gestion source}

La gestion de source est un processus permettant de gérer différentes
version de la même information. Dans sa forme la plus simple, c'est
quelquechose que tout le monde fait manuellement : quand vous modifiez
un fichier, vous le sauvegarder sous un nouveau nom contenant un numéro,
à chaque fois plus grand la précédente version.

Ce genre de gestion de version manuel est cependant sujette facilement
à des erreurs, ainsi, depuis longtemps, des logiciels existent pour
adresser cette problématique. Les premiers outils de gestion de source
étaient destinés à aider un seul utilisateur, à automatiser la gestion
des versions d'un seulf fichier. Dans les dernières décades, cette cilble 
a largement était agrandie, ils gèrent désormais de multiple fichiers, et
aident un grand nombre de personnes à travailler ensemble. Le outils les
plus modernes n'ont aucune difficultés à gérer plusieurs milliers de 
personnes travaillant ensemble sur des projets regroupant plusieurs 
centaines de milliers de fichiers.

\subsection{Pourquoi utiliser un gestionnaire de source ?}

Il y a de nombreuse raisons pour que vous ou votre équipe souhaitiez
utiliser un outil automatisant la gestion de version pour votre projet.
\begin{itemize}
\item L'outil se chargera de suivre l'évolution de votre projet, sans
que vous ayez à le faire. Pour chaque modification, vous aurez à votre
disposition un journal indiquant \emph{qui} a faient quoi, \emph{pourquoi}
ils l'ont fait, \emph{quand} ils l'ont fait, et \emph{ce} qu'ils ont
modifiés.
\item Quand vous travaillez avec d'autres personnes, les logiciels de 
gestion de source facilite le travail collaboratif. Par exemple, quand
plusieurs personnes font, plus ou moins simultannéement, des modifications
incompatibles, le logiciel vous aidera à identifier et résoudre les conflits.
\item L'outil vous aidera à réparer vos erreurs. Si vous effectuez un changement
qui se révèlera être une erreur, vous pourrez revenir fiablement à une version
antérieur d'une fichier ou même d'un ensemble de fichier. En fait, un outil de
gestion de source \emph{vraiment} efficace vous permettra d'identifier à quel
moment le problème est apparu (voir la section~\ref{sec:undo:bisect} pour plus
de détails).
\item L'outil vous permettra aussi de travailler sur plusieurs versions différentes
de votre projet et à gérer l'écart entre chaque.
\end{itemize}
La plupart de ces raisons ont autant d'importances---du moins en théorie--- que
vous travailliez sur un projet pour vous, ou avec une centaine d'autres
personnes.

Une question fondamental à propos des outils de gestion de source, qu'il s'agisse
du projet d'une personne ou d'une grande équipe, est quelles sont ses  
\emph{avantages} par rapport à ses \emph{coût}. Un outil qui est difficile à 
utiliser ou à comprendre exigera un effort d'adoption.

Un projet de cinq milles personnnes s'effondrera très certainement de lui même
sans aucun processus et outil de gestion de source. Dans ce cas, le coût 
d'utilisation d'un logiciel de gestion de source est dérisoire, puisque 
\emph{sans}, l'échec est presque garanti.

D'un autre coté, un ``rapide hack'' d'une personnne peut sembler un contexte
bien pauvre pour utiliser un outil de gestion de source, car, bien évidement
le coût d'utilisation dépasse le coût total du projet. N'est ce pas ?

Mercurial supporte ces \emph{deux} échelles de travail. Vous pouvez apprendre
les bases en juste quelques minutes, et, grâce à sa performance, vous pouvez
l'utiliser avec facilité sur le plus petit des projets. Cette simplicité 
signifie que vous n'avez pas de concepts obscures ou de séquence de commandes
défiant l'imagination, complètement décorrelé de \emph{ce que vous êtes 
vraiment entrain de faire}. En même temps, ces mêmes performances et sa 
nature ``peer-to-peer'' vous permet d'augmenter, sans difficulté, son 
utilisation à de très grand projet.

Aucun outil de gestion de source ne peut sauver un projet mal mené, mais un
bon outil peut faire une grande différence dans la fluidité avec lequel 
vous pourrez travailler avec.

\subsection{Les multiples noms de la gestion de source}

La gestion de source est un domaine divers, tellement qu'il n'existe pas
une seul nom ou acronyme pour le désigner. Voilà quelqu'uns des noms ou 
acronymes que vous rencontrerez le plus souvent:
\begin{itemize}
\item \textit{Revision control (RCS)} ;
\item Software configuration management (SCM), ou \textit{configuration management} ;
\item \textit{Source code management} ;
\item \textit{Source code control}, ou \textit{source control} ;
\item \textit{Version control (VCS)}.
\end{itemize}

\notebox {
Note du traducteur : J'ai conservé la liste des noms en anglais pour des raisons de commodité (ils sont plus ``googelable''). J'ai choisi de conserver le terme ``gestion de sources'' comme traduction unique dans l'ensemble du document.

En outre, j'ai opté pour conserver l'ensemble des opérations de Mercurial (commit, push, pull,...) en anglais, là aussi pour faciliter la lecture d'autres documents en anglais, et 
aussi son utilisation.
}

Certains personnes prétendent que ces termes ont en fait des sens
différents mais en pratique ils se recouvrent tellement qu'il n'y a pas
réellement de manière pertinente de les distinguer.

\section{Une courte histoire de la gestion de source}

Le plus célèbre des anciens outils de gestion de source est \textit{SCCS (Source
Code Control System)}, que Marc Rochkind conçu dans les laboratoire de recherche de Bell 
(\textit{Bell Labs}), dans le début des années 70. \textit{SCCS} ne fonctionner que sur des fichiers individuels, et demandait à personne travaillant sur le projet d'avoir un accès à un répertoire de travail commun, sur un unique système.
Seulement une personne pouvait modifier un fichier au même moment, ce fonctionnement était assuré par l'utilisation de verrou (``lock''). Il était courant que des personnes ne vérouille
des fichiers, et plus tard, oublie de le dévérouiller; empêchant  n'importe qui d'autre de 
travailler sur ces fichiers sans l'aide de l'administrateur...

Walter Tichy a développé une alternative libre à \textit{SCCS} au début des années 80, qu'il
nomma \textit{RSC (Revison Control System)}.  Comme \textit{SCCS}, \textit{RCS}
demander aux développeurs de travailler sur le même répertoire partagé, et de vérouiller les
fichiers pour se prémunir de tout conflit issue de modifications concurrentes.

Un peu plus tard dans les années 1980, Dick Grune utilisa \textit{RCS} comme une brique de base pour un ensemble de scripts \textit{shell} qu'il intitula cmt, avant de la renommer en \textit{CVS (Concurrent Versions System)}.  La grande innovation de CVS était que les développeurs pouvaient travailler simultanéement and indépendament dans leur propre espace de travail. Ces espaces de travail privés assuraient que les développeurs ne se marche mutuellement sur les pieds, comme c'était souvent le cas avec RCS et SCCS. Chaque développeur disposait donc de sa copie de tout les fichiers du projet, et ils pouvaient donc librement les modifier. Ils devaient néanmoins effectuer la ``fusion'' (\textit{``merge''}) de leur fichiers, avant d'effectuer le ``commit'' de leur modification sur le dépôt central.

Brian Berliner took Grune's original scripts and rewrote them in~C,
releasing in 1989 the code that has since developed into the modern
version of CVS.  CVS subsequently acquired the ability to operate over
a network connection, giving it a client/server architecture.  CVS's
architecture is centralised; only the server has a copy of the history
of the project.  Client workspaces just contain copies of recent
versions of the project's files, and a little metadata to tell them
where the server is.  CVS has been enormously successful; it is
probably the world's most widely used revision control system.

In the early 1990s, Sun Microsystems developed an early distributed
revision control system, called TeamWare.  A TeamWare workspace
contains a complete copy of the project's history.  TeamWare has no
notion of a central repository.  (CVS relied upon RCS for its history
storage; TeamWare used SCCS.)

As the 1990s progressed, awareness grew of a number of problems with
CVS.  It records simultaneous changes to multiple files individually,
instead of grouping them together as a single logically atomic
operation.  It does not manage its file hierarchy well; it is easy to
make a mess of a repository by renaming files and directories.  Worse,
its source code is difficult to read and maintain, which made the
``pain level'' of fixing these architectural problems prohibitive.

In 2001, Jim Blandy and Karl Fogel, two developers who had worked on
CVS, started a project to replace it with a tool that would have a
better architecture and cleaner code.  The result, Subversion, does
not stray from CVS's centralised client/server model, but it adds
multi-file atomic commits, better namespace management, and a number
of other features that make it a generally better tool than CVS.
Since its initial release, it has rapidly grown in popularity.

More or less simultaneously, Graydon Hoare began working on an
ambitious distributed revision control system that he named Monotone.
While Monotone addresses many of CVS's design flaws and has a
peer-to-peer architecture, it goes beyond earlier (and subsequent)
revision control tools in a number of innovative ways.  It uses
cryptographic hashes as identifiers, and has an integral notion of
``trust'' for code from different sources.

Mercurial began life in 2005.  While a few aspects of its design are
influenced by Monotone, Mercurial focuses on ease of use, high
performance, and scalability to very large projects.

\section{Trends in revision control}

There has been an unmistakable trend in the development and use of
revision control tools over the past four decades, as people have
become familiar with the capabilities of their tools and constrained
by their limitations.

The first generation began by managing single files on individual
computers.  Although these tools represented a huge advance over
ad-hoc manual revision control, their locking model and reliance on a
single computer limited them to small, tightly-knit teams.

The second generation loosened these constraints by moving to
network-centered architectures, and managing entire projects at a
time.  As projects grew larger, they ran into new problems.  With
clients needing to talk to servers very frequently, server scaling
became an issue for large projects.  An unreliable network connection
could prevent remote users from being able to talk to the server at
all.  As open source projects started making read-only access
available anonymously to anyone, people without commit privileges
found that they could not use the tools to interact with a project in
a natural way, as they could not record their changes.

The current generation of revision control tools is peer-to-peer in
nature.  All of these systems have dropped the dependency on a single
central server, and allow people to distribute their revision control
data to where it's actually needed.  Collaboration over the Internet
has moved from constrained by technology to a matter of choice and
consensus.  Modern tools can operate offline indefinitely and
autonomously, with a network connection only needed when syncing
changes with another repository.

\section{A few of the advantages of distributed revision control}

Even though distributed revision control tools have for several years
been as robust and usable as their previous-generation counterparts,
people using older tools have not yet necessarily woken up to their
advantages.  There are a number of ways in which distributed tools
shine relative to centralised ones.

For an individual developer, distributed tools are almost always much
faster than centralised tools.  This is for a simple reason: a
centralised tool needs to talk over the network for many common
operations, because most metadata is stored in a single copy on the
central server.  A distributed tool stores all of its metadata
locally.  All else being equal, talking over the network adds overhead
to a centralised tool.  Don't underestimate the value of a snappy,
responsive tool: you're going to spend a lot of time interacting with
your revision control software.

Distributed tools are indifferent to the vagaries of your server
infrastructure, again because they replicate metadata to so many
locations.  If you use a centralised system and your server catches
fire, you'd better hope that your backup media are reliable, and that
your last backup was recent and actually worked.  With a distributed
tool, you have many backups available on every contributor's computer.

The reliability of your network will affect distributed tools far less
than it will centralised tools.  You can't even use a centralised tool
without a network connection, except for a few highly constrained
commands.  With a distributed tool, if your network connection goes
down while you're working, you may not even notice.  The only thing
you won't be able to do is talk to repositories on other computers,
something that is relatively rare compared with local operations.  If
you have a far-flung team of collaborators, this may be significant.

\subsection{Advantages for open source projects}

If you take a shine to an open source project and decide that you
would like to start hacking on it, and that project uses a distributed
revision control tool, you are at once a peer with the people who
consider themselves the ``core'' of that project.  If they publish
their repositories, you can immediately copy their project history,
start making changes, and record your work, using the same tools in
the same ways as insiders.  By contrast, with a centralised tool, you
must use the software in a ``read only'' mode unless someone grants
you permission to commit changes to their central server.  Until then,
you won't be able to record changes, and your local modifications will
be at risk of corruption any time you try to update your client's view
of the repository.

\subsubsection{The forking non-problem}

It has been suggested that distributed revision control tools pose
some sort of risk to open source projects because they make it easy to
``fork'' the development of a project.  A fork happens when there are
differences in opinion or attitude between groups of developers that
cause them to decide that they can't work together any longer.  Each
side takes a more or less complete copy of the project's source code,
and goes off in its own direction.

Sometimes the camps in a fork decide to reconcile their differences.
With a centralised revision control system, the \emph{technical}
process of reconciliation is painful, and has to be performed largely
by hand.  You have to decide whose revision history is going to
``win'', and graft the other team's changes into the tree somehow.
This usually loses some or all of one side's revision history.

What distributed tools do with respect to forking is they make forking
the \emph{only} way to develop a project.  Every single change that
you make is potentially a fork point.  The great strength of this
approach is that a distributed revision control tool has to be really
good at \emph{merging} forks, because forks are absolutely
fundamental: they happen all the time.  

If every piece of work that everybody does, all the time, is framed in
terms of forking and merging, then what the open source world refers
to as a ``fork'' becomes \emph{purely} a social issue.  If anything,
distributed tools \emph{lower} the likelihood of a fork:
\begin{itemize}
\item They eliminate the social distinction that centralised tools
  impose: that between insiders (people with commit access) and
  outsiders (people without).
\item They make it easier to reconcile after a social fork, because
  all that's involved from the perspective of the revision control
  software is just another merge.
\end{itemize}

Some people resist distributed tools because they want to retain tight
control over their projects, and they believe that centralised tools
give them this control.  However, if you're of this belief, and you
publish your CVS or Subversion repositories publically, there are
plenty of tools available that can pull out your entire project's
history (albeit slowly) and recreate it somewhere that you don't
control.  So while your control in this case is illusory, you are
forgoing the ability to fluidly collaborate with whatever people feel
compelled to mirror and fork your history.

\subsection{Advantages for commercial projects}

Many commercial projects are undertaken by teams that are scattered
across the globe.  Contributors who are far from a central server will
see slower command execution and perhaps less reliability.  Commercial
revision control systems attempt to ameliorate these problems with
remote-site replication add-ons that are typically expensive to buy
and cantankerous to administer.  A distributed system doesn't suffer
from these problems in the first place.  Better yet, you can easily
set up multiple authoritative servers, say one per site, so that
there's no redundant communication between repositories over expensive
long-haul network links.

Centralised revision control systems tend to have relatively low
scalability.  It's not unusual for an expensive centralised system to
fall over under the combined load of just a few dozen concurrent
users.  Once again, the typical response tends to be an expensive and
clunky replication facility.  Since the load on a central server---if
you have one at all---is many times lower with a distributed
tool (because all of the data is replicated everywhere), a single
cheap server can handle the needs of a much larger team, and
replication to balance load becomes a simple matter of scripting.

If you have an employee in the field, troubleshooting a problem at a
customer's site, they'll benefit from distributed revision control.
The tool will let them generate custom builds, try different fixes in
isolation from each other, and search efficiently through history for
the sources of bugs and regressions in the customer's environment, all
without needing to connect to your company's network.

\section{Why choose Mercurial?}

Mercurial has a unique set of properties that make it a particularly
good choice as a revision control system.
\begin{itemize}
\item It is easy to learn and use.
\item It is lightweight.
\item It scales excellently.
\item It is easy to customise.
\end{itemize}

If you are at all familiar with revision control systems, you should
be able to get up and running with Mercurial in less than five
minutes.  Even if not, it will take no more than a few minutes
longer.  Mercurial's command and feature sets are generally uniform
and consistent, so you can keep track of a few general rules instead
of a host of exceptions.

On a small project, you can start working with Mercurial in moments.
Creating new changes and branches; transferring changes around
(whether locally or over a network); and history and status operations
are all fast.  Mercurial attempts to stay nimble and largely out of
your way by combining low cognitive overhead with blazingly fast
operations.

The usefulness of Mercurial is not limited to small projects: it is
used by projects with hundreds to thousands of contributors, each
containing tens of thousands of files and hundreds of megabytes of
source code.

If the core functionality of Mercurial is not enough for you, it's
easy to build on.  Mercurial is well suited to scripting tasks, and
its clean internals and implementation in Python make it easy to add
features in the form of extensions.  There are a number of popular and
useful extensions already available, ranging from helping to identify
bugs to improving performance.

\section{Mercurial compared with other tools}

Before you read on, please understand that this section necessarily
reflects my own experiences, interests, and (dare I say it) biases.  I
have used every one of the revision control tools listed below, in
most cases for several years at a time.


\subsection{Subversion}

Subversion is a popular revision control tool, developed to replace
CVS.  It has a centralised client/server architecture.

Subversion and Mercurial have similarly named commands for performing
the same operations, so if you're familiar with one, it is easy to
learn to use the other.  Both tools are portable to all popular
operating systems.

Prior to version 1.5, Subversion had no useful support for merges.
At the time of writing, its merge tracking capability is new, and known to be
\href{http://svnbook.red-bean.com/nightly/en/svn.branchmerge.advanced.html#svn.branchmerge.advanced.finalword}{complicated
  and buggy}.

Mercurial has a substantial performance advantage over Subversion on
every revision control operation I have benchmarked.  I have measured
its advantage as ranging from a factor of two to a factor of six when
compared with Subversion~1.4.3's \emph{ra\_local} file store, which is
the fastest access method available.  In more realistic deployments
involving a network-based store, Subversion will be at a substantially
larger disadvantage.  Because many Subversion commands must talk to
the server and Subversion does not have useful replication facilities,
server capacity and network bandwidth become bottlenecks for modestly
large projects.

Additionally, Subversion incurs substantial storage overhead to avoid
network transactions for a few common operations, such as finding
modified files (\texttt{status}) and displaying modifications against
the current revision (\texttt{diff}).  As a result, a Subversion
working copy is often the same size as, or larger than, a Mercurial
repository and working directory, even though the Mercurial repository
contains a complete history of the project.

Subversion is widely supported by third party tools.  Mercurial
currently lags considerably in this area.  This gap is closing,
however, and indeed some of Mercurial's GUI tools now outshine their
Subversion equivalents.  Like Mercurial, Subversion has an excellent
user manual.

Because Subversion doesn't store revision history on the client, it is
well suited to managing projects that deal with lots of large, opaque
binary files.  If you check in fifty revisions to an incompressible
10MB file, Subversion's client-side space usage stays constant The
space used by any distributed SCM will grow rapidly in proportion to
the number of revisions, because the differences between each revision
are large.

In addition, it's often difficult or, more usually, impossible to
merge different versions of a binary file.  Subversion's ability to
let a user lock a file, so that they temporarily have the exclusive
right to commit changes to it, can be a significant advantage to a
project where binary files are widely used.

Mercurial can import revision history from a Subversion repository.
It can also export revision history to a Subversion repository.  This
makes it easy to ``test the waters'' and use Mercurial and Subversion
in parallel before deciding to switch.  History conversion is
incremental, so you can perform an initial conversion, then small
additional conversions afterwards to bring in new changes.


\subsection{Git}

Git is a distributed revision control tool that was developed for
managing the Linux kernel source tree.  Like Mercurial, its early
design was somewhat influenced by Monotone.

Git has a very large command set, with version~1.5.0 providing~139
individual commands.  It has something of a reputation for being
difficult to learn.  Compared to Git, Mercurial has a strong focus on
simplicity.

In terms of performance, Git is extremely fast.  In several cases, it
is faster than Mercurial, at least on Linux, while Mercurial performs
better on other operations.  However, on Windows, the performance and
general level of support that Git provides is, at the time of writing,
far behind that of Mercurial.

While a Mercurial repository needs no maintenance, a Git repository
requires frequent manual ``repacks'' of its metadata.  Without these,
performance degrades, while space usage grows rapidly.  A server that
contains many Git repositories that are not rigorously and frequently
repacked will become heavily disk-bound during backups, and there have
been instances of daily backups taking far longer than~24 hours as a
result.  A freshly packed Git repository is slightly smaller than a
Mercurial repository, but an unpacked repository is several orders of
magnitude larger.

The core of Git is written in C.  Many Git commands are implemented as
shell or Perl scripts, and the quality of these scripts varies widely.
I have encountered several instances where scripts charged along
blindly in the presence of errors that should have been fatal.

Mercurial can import revision history from a Git repository.


\subsection{CVS}

CVS is probably the most widely used revision control tool in the
world.  Due to its age and internal untidiness, it has been only
lightly maintained for many years.

It has a centralised client/server architecture.  It does not group
related file changes into atomic commits, making it easy for people to
``break the build'': one person can successfully commit part of a
change and then be blocked by the need for a merge, causing other
people to see only a portion of the work they intended to do.  This
also affects how you work with project history.  If you want to see
all of the modifications someone made as part of a task, you will need
to manually inspect the descriptions and timestamps of the changes
made to each file involved (if you even know what those files were).

CVS has a muddled notion of tags and branches that I will not attempt
to even describe.  It does not support renaming of files or
directories well, making it easy to corrupt a repository.  It has
almost no internal consistency checking capabilities, so it is usually
not even possible to tell whether or how a repository is corrupt.  I
would not recommend CVS for any project, existing or new.

Mercurial can import CVS revision history.  However, there are a few
caveats that apply; these are true of every other revision control
tool's CVS importer, too.  Due to CVS's lack of atomic changes and
unversioned filesystem hierarchy, it is not possible to reconstruct
CVS history completely accurately; some guesswork is involved, and
renames will usually not show up.  Because a lot of advanced CVS
administration has to be done by hand and is hence error-prone, it's
common for CVS importers to run into multiple problems with corrupted
repositories (completely bogus revision timestamps and files that have
remained locked for over a decade are just two of the less interesting
problems I can recall from personal experience).

Mercurial can import revision history from a CVS repository.


\subsection{Commercial tools}

Perforce has a centralised client/server architecture, with no
client-side caching of any data.  Unlike modern revision control
tools, Perforce requires that a user run a command to inform the
server about every file they intend to edit.

The performance of Perforce is quite good for small teams, but it
falls off rapidly as the number of users grows beyond a few dozen.
Modestly large Perforce installations require the deployment of
proxies to cope with the load their users generate.


\subsection{Choosing a revision control tool}

With the exception of CVS, all of the tools listed above have unique
strengths that suit them to particular styles of work.  There is no
single revision control tool that is best in all situations.

As an example, Subversion is a good choice for working with frequently
edited binary files, due to its centralised nature and support for
file locking.

I personally find Mercurial's properties of simplicity, performance,
and good merge support to be a compelling combination that has served
me well for several years.


\section{Switching from another tool to Mercurial}

Mercurial is bundled with an extension named \hgext{convert}, which
can incrementally import revision history from several other revision
control tools.  By ``incremental'', I mean that you can convert all of
a project's history to date in one go, then rerun the conversion later
to obtain new changes that happened after the initial conversion.

The revision control tools supported by \hgext{convert} are as
follows:
\begin{itemize}
\item Subversion
\item CVS
\item Git
\item Darcs
\end{itemize}

In addition, \hgext{convert} can export changes from Mercurial to
Subversion.  This makes it possible to try Subversion and Mercurial in
parallel before committing to a switchover, without risking the loss
of any work.

The \hgxcmd{conver}{convert} command is easy to use.  Simply point it
at the path or URL of the source repository, optionally give it the
name of the destination repository, and it will start working.  After
the initial conversion, just run the same command again to import new
changes.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "00book"
%%% End: 
