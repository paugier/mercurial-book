<!-- vim: set filetype=docbkxml shiftwidth=2 autoindent expandtab tw=77 : -->

<chapter id="chap:scaling">
  <?dbhtml filename="scaling-mercurial.html"?>
  <title>Scaling Mercurial</title>

  <sect1>
    <title>Scalability issues</title>

    <para>Mercurial has been used in deployments containing over 100 000 files
    and 200 000 changesets, <ulink
	  url="https://mercurial.selenic.com/wiki/BigRepositories">
      still working very well</ulink>.
    It has received, release after release, improvements to scalability.</para>

    <para>Even so, some repositories manage to outgrow
    the default Mercurial setup.</para>

    <para>One possible scaling issue is due to a very large amount
    of changesets. Mercurial stores history efficiently,
    but repository size does grow as more and more history accumulates.
    This can result in slower clones and pulls, as the amount of data
    to download keeps increasing.
    We can handle this issue using so-called "shallow clones".</para>

    <para>A second issue is handling binary files. Changes to binary files
    are not stored as efficiently as changes to text files,
    which results in the repository growing very fast.
    This also results in slower clones and pulls.
    This problem can be tackled with the largefiles extension.</para>

    <para>Repositories with hundreds of thousands of files
    can also pose scalability issues.
    Some common Mercurial commands (like 'hg status') need to check
    all of the files in the repository.
    This is almost not noticeable on small repositories,
    but can become an issue if you have a lot of files.
    The hgwatchman extension automatically detects
    and remembers repository changes, to avoid a slowdown.
    Additionally, if you don't require all files in the repository,
    the narrowhg extension allows you
    to only clone the subset of files you are interested in.</para>

    <para>Finally, a very branchy history can also impact performance.
    Specifically, Mercurial by default does not efficiently store changes
    in lots of different branches developed at the same time.
    As a result, the size of the history can grow much faster
    than when development is mostly linear.
    Using generaldelta encoding, it's possible to store changes
    more efficiently when using many branches.</para>
  </sect1>

  <sect1>
    <title>Scaling up to many changesets with <literal>remotefilelog</literal> </title>

    <para>You may have wondered, especially if you've used
    centralized version control systems before, if it's really necessary
    to copy all of the history when you make a clone.
    Why not download it on-the-fly?</para>

    <para>Well, in many cases, the history isn't really that big,
    and only the initial clone will take quite a bit of time.
    Additionally, having all of the history locally
    makes a lot of operations (like diff and log) much faster.</para>

    <para>However, if you have an extremely large project
    with lots of developers adding history and an enormous amount
    of changesets (TODO: quantify), this can result in slow pulls
    and a very large amount of disk space being used.
    In this case, the benefits of local history
    may be outweighed by its downsides.</para>

    <para>Luckily, there's a solution: the remotefilelog extension.
    This extension allows you to make 'shallow clones',
    keeping all of the different file versions purely on the server.
    As an example, here are the sizes of the history for
    a large repository (mozilla-central) with and without the full history:</para>

      <itemizedlist>
        <listitem>
          <para>with history: 2256 MB</para>
        </listitem>
	<listitem>
          <para>without history: 557 MB</para>
        </listitem>
      </itemizedlist>

    <para>In other words, this extension results in
    downloading 4 times less data from the server on the initial clone!
    There's still quite a bit of data downloaded,
    but future optimizations of shallow cloning should be able
    to get rid of that as well (TODO: verify).</para>

    <para>To get started with remotefilelog, clone the extension from
    <ulink url="https://bitbucket.org/facebook/remotefilelog">Bitbucket</ulink>
    and add it to your hgrc:</para>

    <programlisting>[extensions]
remotefilelog = /path/to/remotefilelog/remotefilelog</programlisting>

    <para>The remotefilelog extension requires configuration
    both on the server and the client side.
    On the server side, all you need to do
    is enable the server functionality.
    Additionally, you can configure the maximum time data is compressed:</para>

    <programlisting>[remotefilelog]
server = True
#keep cached files for 10 days (default 30)
serverexpiration = 10</programlisting>

    <para>On the client side, the only _required_ option
    is the <literal>cachepath</literal>. This specifies where file versions
    will be cached.</para>

    <para>It's enough to specify the following configuration if you want
    to be able to make shallow clones:</para>

    <programlisting>[remotefilelog]
cachepath = /path/to/hgcache</programlisting>

    <para>Once you've specified all of the configuration options,
    you should be able to make a shallow clone, simply by using the
    <literal>--shallow</literal> flag:</para>

    &interaction.ch15-remotefilelog.clone;

    <para>How do we know it's actually a shallow clone? You can still
    run all regular Mercurial commands, so you might not notice.
    One way to find out is to look into the .hg directory.
    All file history is contained inc
    <programlisting>.hg/store/data</programlisting>, so we should see
    a completely empty directory there:</para>

    &interaction.ch15-remotefilelog.check-shallow;

    <para>You'll only be able to do so for access through ssh,
    other protocols are not yet supported right now.</para>
  </sect1>

  <sect1 id="sec:scaling:largefiles">
    <title>Handle large binaries with the <literal
	role="scaling">largefiles</literal> extension</title>

    <para>Mercurial is very good at managing source code and text files.
    It only needs to store the difference between two versions of the file,
    rather than keeping each version completely.
    This avoids the repository from growing quickly.
    However, what happens if we have to deal with (large) binaries?</para>

    <para>It would appear we're not so lucky: each version of the binary
    is stored without delta compression.
    Add a 10 MB binary to your repository and it grows by 10 MB.
    Change a single byte in that binary and commit your new changeset:
    another 10 MB gets added!
    Additionally, every person that wants to clone your repository
    will have to download every version of the binary,
    which quickly starts to add up.</para>

    <para>Luckily, Mercurial has a solution for this problem:
    the largefiles extension. It was added to Mercurial 2.0 in 2011.
    The extension stores large files (large binaries)
    on a server on the network, rather than in the repository history itself.
    The only information saved in the repository itself
    is a 40-byte hash of the file,
    which is placed in the '.hglf/' subdirectory.
    Largefiles are not downloaded when you pull changes.
    Instead, only the largefiles for a specific revision are downloaded,
    when you update to that revision.
    This way, if you add a new version of your 10 MB binary
    to your repository, it only grows by a few bytes.
    If a new user clones your code and updates to the latest revision,
    they will only need to download one 10 MB binary,
    rather than every single one.</para>

    <para>To enable the largefiles extension,
    simply add the following to your hgrc file:</para>

	<programlisting>[extensions]
largefiles =
    </programlisting>

    <para>If you're concerned one of your users
    will forget to enable the extension, don't worry!
    Upon cloning, an informative error message will show up:</para>


    <programlisting>abort: repository requires features unknown to this Mercurial: largefiles!
(see http://mercurial.selenic.com/wiki/MissingRequirement for more information)</programlisting>

    <para>So how do we start using the largefiles extension
    to manage our large binaries?
    Let's setup a repository and create a large binary file:</para>

    &interaction.ch15-largefiles.init;

    <para>Normally, we would add the 'randomdata' file by simply executing:</para>

    &interaction.ch15-largefiles.add-regular;

    <para>
However, we've enabled the largefiles extension. This allows us to execute:</para>

    &interaction.ch15-largefiles.add-largefile;

    <para>Using the additional '--large' flag,
    we've clarified that we want this file to be stored as a largefile.</para>
    <para>The repository now not only contains the 'randomdata' file,
    it also contains a '.hglf/' directory,
    containing a textfile called 'randomdata'.
    That file in turn contains a 40-byte hash
    that allows Mercurial to know what contents should actually be placed
    in the 'randomdata' file when updating to a specific revision.</para>

    <para>Largefiles are propagated by pushing or pulling.
    If you push new revisions to another repository,
    all of the largefiles changed in those revisions will be pushed as well.
    This allows you to upload all of your largefiles to a central server.</para>

    <para>If you pull new revisions from another repository,
    by default the changed largefiles will not be pulled
    into your local repository!
    That only happens when you update to a revision
    containing the new version of a largefile.
    This ensures you don't have to download huge amounts of data,
    just to have a single version of a largefile available.</para>

    <para>If you want to explicitly get all of the largefiles
    into your repository, you can use lfpull:</para>

    <programlisting>$ hg lfpull --rev relevantrevisions</programlisting>

    <para>Alternatively, you can also use the '--lfrev' flag:</para>
    <programlisting>$ hg pull --lfrev relevantrevisions</programlisting>

    <para>This allows you to easily download all largefiles,
    be it for offline access or for backup purposes.</para>

    <para>Once you've added a single largefile to a repository,
    new files over 10 MB that you add to the repository
    will automatically be added as largefile.
    It's possible to configure your system in a different way,
    using two specific configuration options.</para>

    <itemizedlist>
	  <listitem>
        <para>The largefiles.minsize option allows
        specifying a size (in MB). All new files larger than this size
        will automatically be added as largefile.</para>
      </listitem>
	  <listitem>
        <para>The largefiles.patterns option allow specifying
        regex or glob patterns. All files that match one of the patterns
        will automatically be added as largefile,
        even if they are smaller than largefiles.minsize!</para>
      </listitem>
    </itemizedlist>

    <para>An example configuration:</para>

    <programlisting>[largefiles]
# Add all files over 3 MB as largefile
minsize = 3
# All files matching one of the below patterns will be added as largefile
patterns =
  *.jpg
  re:.*\.(png|bmp)$
  library.zip
  content/audio/*</programlisting>

    <para>The largefiles extension comes with a trade-off.
    It's very useful for scalability, allowing people to use Mercurial
    for large files and binaries
    without letting the repository size grow enormously.
    However, that's exactly where the downside lies as well:
    not all file versions are downloaded automatically when pulling.
    This means the largefiles extension
    removes part of the distributed nature of Mercurial.</para>

    <para>Suppose you are on a plane without network access.
    Can you still update to each revision when largefiles are in use?
    Not necessarily.
    Suppose the disk containing your central repository crashes.
    Can you simply clone from a user repository and carry on?
    Not unless that user repository has all of the largefiles you need.</para>

    <para>In conclusion: the largefiles extension is very useful,
    but keep in mind its downsides before you start using it!</para>
  </sect1>
</chapter>

<!--
local variables: 
sgml-parent-document: ("00book.xml" "book" "chapter")
end:
-->
