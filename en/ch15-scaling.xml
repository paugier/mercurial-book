<!-- vim: set filetype=docbkxml shiftwidth=2 autoindent expandtab tw=77 : -->

<chapter id="chap:scaling">
  <?dbhtml filename="scaling-mercurial.html"?>
  <title>Scaling Mercurial</title>

  <sect1>
    <title>Scalability issues</title>

    <para>Mercurial has been used in deployments containing over 100 000 files
    and 200 000 changesets, <ulink
	  url="https://mercurial.selenic.com/wiki/BigRepositories">
      still working very well</ulink>.
    It has received, release after release, improvements to scalability.</para>

    <para>Even so, some repositories manage to outgrow
    the default Mercurial setup.</para>

    <para>One possible scaling issue is due to a very large amount
    of changesets. Mercurial stores history efficiently,
    but repository size does grow as more and more history accumulates.
    This can result in slower clones and pulls, as the amount of data
    to download keeps increasing.
    We can handle this issue using so-called "shallow clones".</para>

    <para>A second issue is handling binary files. Changes to binary files
    are not stored as efficiently as changes to text files,
    which results in the repository growing very fast.
    This also results in slower clones and pulls.
    This problem can be tackled with the largefiles extension.</para>

    <para>Repositories with hundreds of thousands of files
    can also pose scalability issues.
    Some common Mercurial commands (like 'hg status') need to check
    all of the files in the repository.
    This is almost not noticeable on small repositories,
    but can become an issue if you have a lot of files.
    The hgwatchman extension automatically detects
    and remembers repository changes, to avoid a slowdown.
    Additionally, if you don't require all files in the repository,
    the narrowhg extension allows you
    to only clone the subset of files you are interested in.</para>

    <para>Finally, a very branchy history can also impact performance.
    Specifically, Mercurial by default does not efficiently store changes
    in lots of different branches developed at the same time.
    As a result, the size of the history can grow much faster
    than when development is mostly linear.
    Using generaldelta encoding, it's possible to store changes
    more efficiently when using many branchese.</para>
  </sect1>


  <sect1 id="sec:scaling:largefiles">
    <title>Handle large binaries with the <literal
	role="scaling">largefiles</literal> extension</title>

    <para>Mercurial is very good at managing source code and text files.
    It only needs to store the difference between two versions of the file,
    rather than keeping each version completely.
    This avoids the repository from growing quickly.
    However, what happens if we have to deal with (large) binaries?</para>

    <para>It would appear we're not so lucky: each version of the binary
    is stored without delta compression.
    Add a 10 MB binary to your repository and it grows by 10 MB.
    Change a single byte in that binary and commit your new changeset:
    another 10 MB gets added!
    Additionally, every person that wants to clone your repository
    will have to download every version of the binary,
    which quickly starts to add up.</para>

    <para>Luckily, Mercurial has a solution for this problem:
    the largefiles extension. It was added to Mercurial 2.0 in 2011.
    The extension stores large files (large binaries)
    on a server on the network, rather than in the repository history itself.
    The only information saved in the repository itself
    is a 40-byte hash of the file,
    which is placed in the '.hglf/' subdirectory.
    Largefiles are not downloaded when you pull changes.
    Instead, only the largefiles for a specific revision are downloaded,
    when you update to that revision.
    This way, if you add a new version of your 10 MB binary
    to your repository, it only grows by a few bytes.
    If a new user clones your code and updates to the latest revision,
    they will only need to download one 10 MB binary,
    rather than every single one.</para>

    <para>To enable the largefiles extension,
    simply add the following to your hgrc file:</para>

	<programlisting>[extensions]
largefiles =
    </programlisting>

    <para>If you're concerned one of your users
    will forget to enable the extension, don't worry!
    Upon cloning, an informative error message will show up:</para>


    <programlisting>abort: repository requires features unknown to this Mercurial: largefiles!
(see http://mercurial.selenic.com/wiki/MissingRequirement for more information)</programlisting>

    <para>So how do we start using the largefiles extension
    to manage our large binaries?
    Let's setup a repository and create a large binary file:</para>

    <programlisting>$ hg init foo
$ cd foo
$ dd if=/dev/urandom of=randomdata count=2000</programlisting>

    <para>Normally, we would add the 'randomdata' file by simply executing:</para>

    <programlisting>$ hg add randomdata
$ hg commit -m 'added randomdata as regular file'</programlisting>

    <para>
However, we've enabled the largefiles extension. This allows us to execute:</para>

    <programlisting>$ hg add --large randomdata
$ hg commit -m 'added randomdata as largefile'</programlisting>

    <para>Using the additional '--large' flag,
    we've clarified that we want this file to be stored as a largefile.</para>
    <para>The repository now not only contains the 'randomdata' file,
    it also contains a '.hglf/' directory,
    containing a textfile called 'randomdata'.
    That file in turn contains a 40-byte hash
    that allows Mercurial to know what contents should actually be placed
    in the 'randomdata' file when updating to a specific revision.</para>

    <para>Largefiles are propagated by pushing or pulling.
    If you push new revisions to another repository,
    all of the largefiles changed in those revisions will be pushed as well.
    This allows you to upload all of your largefiles to a central server.</para>

    <para>If you pull new revisions from another repository,
    by default the changed largefiles will not be pulled
    into your local repository!
    That only happens when you update to a revision
    containing the new version of a largefile.
    This ensures you don't have to download huge amounts of data,
    just to have a single version of a largefile available.</para>

    <para>If you want to explicitly get all of the largefiles
    into your repository, you can use lfpull:</para>

    <programlisting>$ hg lfpull --rev relevantrevisions</programlisting>

    <para>Alternatively, you can also use the '--lfrev' flag:</para>
    <programlisting>$ hg pull --lfrev relevantrevisions</programlisting>

    <para>This allows you to easily download all largefiles,
    be it for offline access or for backup purposes.</para>

    <para>Once you've added a single largefile to a repository,
    new files over 10 MB that you add to the repository
    will automatically be added as largefile.
    It's possible to configure your system in a different way,
    using two specific configuration options.</para>

    <itemizedlist>
	  <listitem>
        <para>The largefiles.minsize option allows
        specifying a size (in MB). All new files larger than this size
        will automatically be added as largefile.</para>
      </listitem>
	  <listitem>
        <para>The largefiles.patterns option allow specifying
        regex or glob patterns. All files that match one of the patterns
        will automatically be added as largefile,
        even if they are smaller than largefiles.minsize!</para>
      </listitem>
    </itemizedlist>

    <para>An example configuration:</para>

    <programlisting>[largefiles]
# Add all files over 3 MB as largefile
minsize = 3
# All files matching one of the below patterns will be added as largefile
patterns =
  *.jpg
  re:.*\.(png|bmp)$
  library.zip
  content/audio/*</programlisting>

    <para>The largefiles extension comes with a trade-off.
    It's very useful for scalability, allowing people to use Mercurial
    for large files and binaries
    without letting the repository size grow enormously.
    However, that's exactly where the downside lies as well:
    not all file versions are downloaded automatically when pulling.
    This means the largefiles extension
    removes part of the distributed nature of Mercurial.</para>

    <para>Suppose you are on a plane without network access.
    Can you still update to each revision when largefiles are in use?
    Not necessarily.
    Suppose the disk containing your central repository crashes.
    Can you simply clone from a user repository and carry on?
    Not unless that user repository has all of the largefiles you need.</para>

    <para>In conclusion: the largefiles extension is very useful,
    but keep in mind its downsides before you start using it!</para>
  </sect1>
</chapter>

<!--
local variables: 
sgml-parent-document: ("00book.xml" "book" "chapter")
end:
-->
